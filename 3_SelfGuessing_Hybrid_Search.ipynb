{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e689df44-cdbc-484c-ac4a-0eaa899d6c7a",
   "metadata": {},
   "source": [
    "# Self-Guessing Hybrid Search (aka Part 3/3)\n",
    "\n",
    "Assume we have a working recipe for hybrid search in the form `(query, [keyword]) => [(snippet, siilarity)]`.\n",
    "\n",
    "Now the goal of this notebook is, _can we self-guess the keywords from the query and have the caller supply just the query?_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "221fdcd4-dd8b-48b3-94d1-75cbaf579cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from functools import partial\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "_ = load_dotenv('.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05129e85-c628-495e-b3cc-4fdde3f69154",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cassio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbf13f7c-cbe8-4447-96d9-5eb7aa820874",
   "metadata": {},
   "outputs": [],
   "source": [
    "cassio.init(\n",
    "    token=os.environ['ASTRA_DB_APPLICATION_TOKEN'],\n",
    "    database_id=os.environ['ASTRA_DB_ID'],\n",
    "    keyspace=os.environ.get('ASTRA_DB_KEYSPACE'),\n",
    ")\n",
    "session = cassio.config.resolve_session()\n",
    "keyspace = cassio.config.resolve_keyspace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6115336-6e1f-4365-8244-2e53ea4cf0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "embedding_model_name = \"text-embedding-ada-002\"\n",
    "\n",
    "def get_embeddings(texts):\n",
    "    result = openai.Embedding.create(\n",
    "        input=texts,\n",
    "        engine=embedding_model_name,\n",
    "    )\n",
    "    return [res.embedding for res in result.data]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4a049e-934f-4254-a5ab-36b82dc55647",
   "metadata": {},
   "source": [
    "### Recap\n",
    "\n",
    "Let's use the latest \"hybrid search\" function from the previous investigation. One important point is that we'll proceed with the keywords in OR (i.e. a single keyword match suffices for a hit). This enables a meaningful contribution to the \"score\" from the keyword side; but most important, with the keywords being self-guessed, it protects somewhat from too-demanding \"guesses\" from the query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5bc597-b20a-4797-964e-2ed42cb187aa",
   "metadata": {},
   "source": [
    "**We have packaged the latest machinery from Part 2 into a Python module to reduce clutter, nothing new**\n",
    "\n",
    "_(We just renamed the final hybrid-search function for convenience and made sure the DB parameters pass through the calls)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "724ec670-f443-43bf-aae9-cc3ef1cf33d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kw_hybrid_tools import hybrid_search_with_kw, show, keyword_similarity, sum_score_merger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a4f616-5321-476f-9f4b-84d4975cf6e6",
   "metadata": {},
   "source": [
    "That has the following signature:\n",
    "```\n",
    "def hybrid_search_with_kw(session, keyspace, get_embeddings, query, keywords=[],\n",
    "                          top_k=3, kw_similarity_function=keyword_similarity,\n",
    "                          score_merger_function=sum_score_merger, prefetch_factor=5):\n",
    "    ...\n",
    "```\n",
    "\n",
    "Now, let's define a handy shortcut and run a sanity check (to be compared with the \"QUERY7/KW7\" run in the previous notebook):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b2e846a-57a2-491c-8b4b-8bee3b12c499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[with safe prefetch] QUERY: 'How come I cannot chat?', KEYWORDS: 'support, chat'\n",
      "    [1] 0.96761 \"I cannot open the support chat.\"\n",
      "    [2] 0.96156 \"I see no messages in the support chat.\"\n",
      "    [3] 0.95498 \"The support chat on the website is lagging.\"\n"
     ]
    }
   ],
   "source": [
    "hybrid_kw = partial(hybrid_search_with_kw, session=session, keyspace=keyspace, get_embeddings=get_embeddings)\n",
    "\n",
    "KW0 = ['support', 'chat']\n",
    "QUERY0 = \"How come I cannot chat?\"\n",
    "\n",
    "print(f\"[with safe prefetch] QUERY: '{QUERY0}', KEYWORDS: \\'{', '.join(KW0)}\\'\")\n",
    "show(hybrid_kw(query=QUERY0, keywords=KW0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f9becb-040d-41ba-a11f-6f8e17573f89",
   "metadata": {},
   "source": [
    "## Guessing the keywords from the query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f4fa69-fbe6-4702-82ba-b8ae7d467341",
   "metadata": {},
   "source": [
    "Let us consider first a simple and disappointing keyword-guesser function (there'll be several of them) later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8c831c7-2f15-4d87-a152-e70ab864a62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "PUNKT = set('!,.?;\\'\"-+=/[]{}()\\n')\n",
    "\n",
    "def guess_kws_simple(query):\n",
    "    _qry = ''.join([c for c in query if c not in PUNKT]).lower()\n",
    "    return {\n",
    "        w\n",
    "        for w in _qry.split(' ')\n",
    "        if w\n",
    "        if len(w) > 4\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f28ca0-c451-47c6-be13-9a9a6398b378",
   "metadata": {},
   "source": [
    "Rather crude, isn't it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12874151-d83a-492f-bee6-13fe3def389c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'today', 'report', 'benjamin'}\n"
     ]
    }
   ],
   "source": [
    "print(guess_kws_simple(\"The report due today is on Mia's desk, Benjamin!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bdb11f-0754-451c-9f41-b4be7428daed",
   "metadata": {},
   "source": [
    "Let's start from this and repackage a keyword-guessing hybrid search function (again, we take advantage of the partialed shortcut to focus on the important bits):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4434714b-f6f6-47b2-9093-d4f33ea47d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_guess(query, kw_guesser, top_k=3, kw_similarity_function=keyword_similarity,\n",
    "                 score_merger_function=sum_score_merger, prefetch_factor=5):\n",
    "    keywords = kw_guesser(query)\n",
    "    return hybrid_kw(\n",
    "        query=query,\n",
    "        keywords=keywords,\n",
    "        top_k=top_k,\n",
    "        kw_similarity_function=kw_similarity_function,\n",
    "        score_merger_function=score_merger_function,\n",
    "        prefetch_factor=prefetch_factor,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d45c749-4979-48fc-87f0-3ac0863da72d",
   "metadata": {},
   "source": [
    "A little test with the crude guesser (and, let's not bother with the other settings now):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd560dcd-3b01-451d-a59f-5025cfcc8c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUERY: 'How come I cannot chat?' [keywords={'cannot'}]\n",
      "    [1] 0.96762 \"I cannot open the support chat.\"\n",
      "    [2] 0.95840 \"I cannot speak with the support operator!\"\n"
     ]
    }
   ],
   "source": [
    "QUERY1 = \"How come I cannot chat?\"\n",
    "print(f\"QUERY: '{QUERY1}' [keywords={guess_kws_simple(QUERY1)}]\")\n",
    "show(hybrid_guess(QUERY1, kw_guesser=guess_kws_simple))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ef24ac-11f9-4ce5-8c44-3db12bdd4348",
   "metadata": {},
   "source": [
    "But clearly this is not the best solution in general:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b35d59f5-6103-4f61-94dc-c819cd40fdf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUERY: 'Do you currently have any offers?' [keywords={'offers', 'currently'}]\n",
      "QUERY: 'Why does the site experience these lags?' [keywords={'experience', 'these'}]\n"
     ]
    }
   ],
   "source": [
    "QUERY2 = \"Do you currently have any offers?\"\n",
    "print(f\"QUERY: '{QUERY2}' [keywords={guess_kws_simple(QUERY2)}]\")\n",
    "show(hybrid_guess(QUERY2, kw_guesser=guess_kws_simple))\n",
    "\n",
    "QUERY3 = \"Why does the site experience these lags?\"\n",
    "print(f\"QUERY: '{QUERY3}' [keywords={guess_kws_simple(QUERY3)}]\")\n",
    "show(hybrid_guess(QUERY3, kw_guesser=guess_kws_simple))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed0fb44-2cd6-4288-98b8-c166140ec890",
   "metadata": {},
   "source": [
    "_Note: with just \"Why does the site lag?\" you **would** get some results ... of course! No keywords found, and it falls back to ANN-only._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1c1fbb-7564-4667-9180-8fcaf587afad",
   "metadata": {},
   "source": [
    "### Known keyword set, quick approaches\n",
    "\n",
    "Suppose your knowledge of the problem domain lets you make an explicit list of the keywords you want to potentially use:\n",
    "\n",
    "> You may achieve this \"by hand\", or by passing a random sample subset of the snippets to an LLM, ... or a combination of both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bab263c6-64f6-4b73-9b8c-cb970eb02a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "available_keywords = set(\"buy gift discounts support operator chat message offer lag product payment process shop cart\".split(\" \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd36da5-2d01-48ca-9f16-3039761f52fe",
   "metadata": {},
   "source": [
    "Now very \"cheap\" but in some cases effective (and fast!) keyword extractors can be constructed.\n",
    "\n",
    "_Note: sometimes these might yield false positives, such as \"lag\" being a substring of \"flagged\". The semantic side of the search would mostly take care of these, with the proper tuning in the score-merging phase._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c5901ec1-7056-4a87-b62d-96d4c2ffce98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def guess_kws_substr_from_set(query, kws=available_keywords):\n",
    "    _qry = query.lower().strip()\n",
    "    return {kw for kw in available_keywords if kw in _qry}\n",
    "\n",
    "def guess_kws_tokens_from_set(query, kws=available_keywords):\n",
    "    _qry = ''.join([c for c in query if c not in PUNKT]).lower()\n",
    "    toks = {tk for tk in _qry.split(' ') if tk}\n",
    "    return toks & kws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c793b269-294a-4f3d-98c6-7556ee5c4e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "QUERY='How come I cannot chat?'\n",
      "  KwGuesser=<SUBSTR, from set> [keywords={'chat'}]\n",
      "    [1] 0.96762 \"I cannot open the support chat.\"\n",
      "    [2] 0.96608 \"A message disappeared from the chat?\"\n",
      "    [3] 0.96157 \"I see no messages in the support chat.\"\n",
      "  KwGuesser=<TOKENS, from set> [keywords={'chat'}]\n",
      "    [1] 0.96762 \"I cannot open the support chat.\"\n",
      "    [2] 0.96608 \"A message disappeared from the chat?\"\n",
      "    [3] 0.96157 \"I see no messages in the support chat.\"\n",
      "\n",
      "QUERY='Do you currently have any offers?'\n",
      "  KwGuesser=<SUBSTR, from set> [keywords={'offer'}]\n",
      "    [1] 0.96833 \"Is there any special offer today?\"\n",
      "    [2] 0.47047 \"Are special offers available?\"\n",
      "  KwGuesser=<TOKENS, from set> [keywords=set()]\n",
      "    [1] 0.97047 \"Are special offers available?\"\n",
      "    [2] 0.96833 \"Is there any special offer today?\"\n",
      "    [3] 0.94816 \"I would like to buy gift cards. Where can I get discounts?\"\n",
      "\n",
      "QUERY='Why does the site experience these lags?'\n",
      "  KwGuesser=<SUBSTR, from set> [keywords={'lag'}]\n",
      "    [1] 0.46398 \"The support chat on the website is lagging.\"\n",
      "  KwGuesser=<TOKENS, from set> [keywords=set()]\n",
      "    [1] 0.96399 \"The support chat on the website is lagging.\"\n",
      "    [2] 0.94488 \"I am having trouble opening my shopping cart!\"\n",
      "    [3] 0.94375 \"An operator chats with several people at the same time?\"\n"
     ]
    }
   ],
   "source": [
    "queries = [QUERY1, QUERY2, QUERY3]\n",
    "guessers = [('SUBSTR, from set', guess_kws_substr_from_set), ('TOKENS, from set', guess_kws_tokens_from_set)]\n",
    "\n",
    "for qry in queries:\n",
    "    print(f\"\\nQUERY='{qry}'\")\n",
    "    for kw_g_name, kw_g in guessers:\n",
    "        print(f\"  KwGuesser=<{kw_g_name}> [keywords={kw_g(qry)}]\")\n",
    "        show(hybrid_guess(qry, kw_guesser=kw_g))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc79d335-ef0d-4248-8641-7580f1ffdbfe",
   "metadata": {},
   "source": [
    "The substring mode seems to fare better: we get `\"lag\"` (which was too short in the crude approach) and we don't get confused by `\"cannot\"` or similar irrelevant things. _Note: the many results from the token mode for the second and third query signal that no keywords were found, don't let that fool you._\n",
    "\n",
    "As remarked earlier, however we may get false positives (especially if there aren't better vectors in the store, to climb to the top results and displace the intruders):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "120d475e-9529-4e4c-a57b-48ea9520bd12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "KwGuesser=<SUBSTR, from set>, QUERY='I have been flagged by an admin... what do I do?' [keywords={'lag'}]\n",
      "    [1] 0.43556 \"The support chat on the website is lagging.\"\n"
     ]
    }
   ],
   "source": [
    "QUERY4 = \"I have been flagged by an admin... what do I do?\"\n",
    "print(f\"\\nKwGuesser=<SUBSTR, from set>, QUERY='{QUERY4}' [keywords={guess_kws_substr_from_set(QUERY4)}]\")\n",
    "show(hybrid_guess(QUERY4, kw_guesser=guess_kws_substr_from_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682a6112-0f81-4415-bdf9-b900301e1bec",
   "metadata": {},
   "source": [
    "### Known keyword set, using AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a828a6-00de-4a46-8823-93850ae1bcae",
   "metadata": {},
   "source": [
    "Of course, the next thing we try is to employ AI to nail the kewords for us. This, however, comes at a performance cost. Depending on the use cases, an additional delay of one second or more might be acceptable or not. Let's see what we can do, and defer timing the performance to a later section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d107f7-0bfa-472f-b07b-3d18bdf4b6ee",
   "metadata": {},
   "source": [
    "#### HuggingFace zero-shot classifier\n",
    "\n",
    "There's a new parameter here, a threshold to accept the keyword from the classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745d7562-3320-4d80-aa69-e4224f91fda9",
   "metadata": {},
   "source": [
    "> Install pytorch. This is not covered in the `requirements.txt` since it's ... complicated. Please follow [this](https://pytorch.org/get-started/locally/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9af5d81-072a-49f8-8d8b-ccadfdb5173d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to facebook/bart-large-mnli and revision c626438 (https://huggingface.co/facebook/bart-large-mnli).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "hf_zs_classifier = pipeline(\"zero-shot-classification\")\n",
    "\n",
    "def guess_kws_hf_zs_from_set(query, kws=available_keywords, keyword_threshold=0.5):\n",
    "    _kws = list(kws)\n",
    "    result = hf_zs_classifier([query], _kws, multi_label=True)[0]\n",
    "    return {\n",
    "        keyword\n",
    "        for keyword, kw_score in zip(result[\"labels\"], result[\"scores\"])\n",
    "        if kw_score >= keyword_threshold\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d576ed-b08e-4148-a8fa-bf112211b4bc",
   "metadata": {},
   "source": [
    "Just the keyword extraction in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c0c7ca09-d65a-4e10-a796-acf3be45defd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUERY='How come I cannot chat?' [keywords={'lag'}]\n",
      "QUERY='Do you currently have any offers?' [keywords={'discounts', 'operator', 'message', 'offer'}]\n",
      "QUERY='Why does the site experience these lags?' [keywords={'lag', 'operator'}]\n",
      "QUERY='I have been flagged by an admin... what do I do?' [keywords={'process', 'chat', 'operator', 'message'}]\n"
     ]
    }
   ],
   "source": [
    "queries = [QUERY1, QUERY2, QUERY3, QUERY4]\n",
    "\n",
    "for qry in queries:\n",
    "    print(f\"QUERY='{qry}' [keywords={guess_kws_hf_zs_from_set(qry)}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93693dd-5c31-4f05-a002-013a8bc439e8",
   "metadata": {},
   "source": [
    "Remember you can set the extraction to be more or less generous by playing with the threshold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2c6a7c1a-d7c6-4211-9dc9-ee9ebfcc124a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUERY='I have been flagged by an admin... what do I do?', keywords by threshold for guess_kws_hf_zs:\n",
      "    threshold=0.30 ==> [keywords={'chat', 'process', 'lag', 'operator', 'message', 'cart', 'offer'}]\n",
      "    threshold=0.40 ==> [keywords={'process', 'chat', 'operator', 'message'}]\n",
      "    threshold=0.50 ==> [keywords={'process', 'chat', 'operator', 'message'}]\n",
      "    threshold=0.70 ==> [keywords={'process', 'operator', 'message'}]\n",
      "    threshold=0.80 ==> [keywords={'process', 'message'}]\n",
      "    threshold=0.90 ==> [keywords=set()]\n"
     ]
    }
   ],
   "source": [
    "print(f\"QUERY='{QUERY4}', keywords by threshold for guess_kws_hf_zs:\")\n",
    "for kw_t in [0.3, 0.4, 0.5, 0.7, 0.8, 0.9]:\n",
    "    print(f\"    threshold={kw_t:0.2f} ==> [keywords={guess_kws_hf_zs_from_set(QUERY4, keyword_threshold=kw_t)}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9826d549-b521-421b-af93-024b57413230",
   "metadata": {},
   "source": [
    "Not very fast (comparable to calling OpenAI or something), but probably a good choice if one wants to run everything locally (and/or save on OpenAI cost, I guess)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df8714c-5624-456a-a969-984d235124f7",
   "metadata": {},
   "source": [
    "#### _(Abandoned)_ HuggingFace LLM to get the keywords from a set\n",
    "\n",
    "This does not seem to lead to any quickly usable result. Abandoned for now (besides, it's slower than calling LLM services).\n",
    "\n",
    "Keeping them for record, one attempt per cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6b5436fd-62f1-456d-b3c0-53fdf786d19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = '''\n",
    "\n",
    "# FROM: https://huggingface.co/docs/transformers/v4.15.0/en/task_summary#text-generation\n",
    "\n",
    "\n",
    "hf_tg_llm = pipeline(\"text-generation\")\n",
    "\n",
    "# just did a few tests with prompts, no luck\n",
    "KW_EXTRACTION_PROMPT_TEMPLATE = \"\"\"The relevant keywords extracted from \"{query}\" are: \"\"\"\n",
    "prompt = KW_EXTRACTION_PROMPT_TEMPLATE.format(query=QUERY3)\n",
    "\n",
    "result = hf_tg_llm(prompt, max_length=50 + len(prompt), do_sample=False)\n",
    "print(result[0]['generated_text'])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "deaed8b0-ea71-4216-ac8b-0ce305bfd0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = '''\n",
    "# FROM: https://huggingface.co/docs/transformers/v4.15.0/en/task_summary#text-generation\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"xlnet-base-cased\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"xlnet-base-cased\")\n",
    "\n",
    "# Padding text helps XLNet with short prompts - proposed by Aman Rusia in https://github.com/rusiaaman/XLNet-gen#methodology\n",
    "PADDING_TEXT = \"\"\"In 1991, the remains of Russian Tsar Nicholas II and his family\n",
    "(except for Alexei and Maria) are discovered.\n",
    "The voice of Nicholas's young son, Tsarevich Alexei Nikolaevich, narrates the\n",
    "remainder of the story. 1883 Western Siberia,\n",
    "a young Grigori Rasputin is asked by his father and a group of men to perform magic.\n",
    "Rasputin has a vision and denounces one of the men as a horse thief. Although his\n",
    "father initially slaps him for making such an accusation, Rasputin watches as the\n",
    "man is chased outside and beaten. Twenty years later, Rasputin sees a vision of\n",
    "the Virgin Mary, prompting him to become a priest. Rasputin quickly becomes famous,\n",
    "with people, even a bishop, begging for his blessing. <eod> </s> <eos>\"\"\"\n",
    "\n",
    "prompt = f\"The top search keyword extracted from \\\"{QUERY4}\\\" are ...\"\n",
    "\n",
    "inputs = tokenizer(PADDING_TEXT + prompt, add_special_tokens=False, return_tensors=\"pt\")[\"input_ids\"]\n",
    "prompt_length = len(tokenizer.decode(inputs[0]))\n",
    "outputs = model.generate(inputs, max_length=250, do_sample=True, top_p=0.95, top_k=60)\n",
    "generated = prompt + tokenizer.decode(outputs[0])[prompt_length+1:]\n",
    "\n",
    "print(generated)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7679d14a-4746-4661-9cba-f33445b8359f",
   "metadata": {},
   "source": [
    "#### Using a greater LLM for keywords from a set\n",
    "\n",
    "What if we switch to a more modern, powerful LLM such as gpt3 from OpenAI, and have it extract our keywords from a set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c160bf7e-51f7-4a64-b36d-1d50d0890329",
   "metadata": {},
   "outputs": [],
   "source": [
    "KW_EXTRACTION_FROM_SET_PROMPT_TEMPLATE = \"\"\"\n",
    "You are to extract keywords from a phrase for use in a keyword-based search engine.\n",
    "The keywords need not appear in the input phrase, meaning you can relate synonyms.\n",
    "Please output them in a comma-separated list.\n",
    "Be very careful that keywords MUST be given in stemmed form: nouns are singular, verbs are in the infinite, and so on.\n",
    "Do not exceed {max_num_keywords} keywords. Keywords must not include whitespaces, i.e. they must be a single word.\n",
    "Include proper nouns if relevant. Discard stop words, pronouns and generic verbs such as be, do, get and so on.\n",
    "Important: limit your selection to a provided pool of available keywords.\n",
    "\n",
    "EXAMPLE INPUT PHRASE: Does the site currently offer discounts? It featured them on the portal yesterday.\n",
    "EXAMPLE AVAILABLE KEYWORDS: sale, discount, portal, reboot, offer, website, malfunction, discount, purchase, technical\n",
    "EXAMPLE KEYWORDS: website, offer, discount, feature, portal\n",
    "\n",
    "INPUT PHRASE: {query}\n",
    "AVAILABLE KEYWORDS: {available_keywords}\n",
    "KEYWORDS:\"\"\"\n",
    "\n",
    "completion_model_name = \"gpt-3.5-turbo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0cdebddd-a932-4413-8967-7bb71b93da9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def guess_kws_gpt3_from_set(query, available_keywords=available_keywords, max_num_keywords=12):\n",
    "    llm_prompt = KW_EXTRACTION_FROM_SET_PROMPT_TEMPLATE.format(\n",
    "        query=query,\n",
    "        available_keywords=', '.join(sorted(available_keywords)),\n",
    "        max_num_keywords=max_num_keywords,\n",
    "    )\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=completion_model_name,\n",
    "        messages=[{\"role\": \"user\", \"content\": llm_prompt}],\n",
    "        temperature=0.0,\n",
    "        max_tokens=40,\n",
    "    )\n",
    "    kws_string = response.choices[0].message.content\n",
    "    kws = {\n",
    "        kw_tok.strip()\n",
    "        for kw_tok in kws_string.split(',')\n",
    "        if kw_tok.strip()\n",
    "    }\n",
    "    # Constraining the LLM to stay in the provided set is not easy at all. We fix it here (not ideal)\n",
    "    return kws & set(available_keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cc0b0d-dfb3-4ea9-837f-eb7d23d4c681",
   "metadata": {},
   "source": [
    "Let's check how this works. We also add a funny query to put the LLM to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e154398d-002b-4e9c-b5b7-eb2ed246bff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available keywords::\n",
      "    {'discounts', 'chat', 'process', 'lag', 'operator', 'payment', 'gift', 'support', 'shop', 'message', 'cart', 'product', 'buy', 'offer'}\n",
      "QUERY='How come I cannot chat?'\n",
      "    keywords => {'chat'}\n",
      "QUERY='Do you currently have any offers?'\n",
      "    keywords => {'offer'}\n",
      "QUERY='Why does the site experience these lags?'\n",
      "    keywords => {'lag'}\n",
      "QUERY='I have been flagged by an admin... what do I do?'\n",
      "    keywords => set()\n",
      "QUERY='Is Santa a user of your website? MY FRIEND'S MESSAGE ASSURES ME SO. Santa gives me a nice present each year.'\n",
      "    keywords => {'message'}\n"
     ]
    }
   ],
   "source": [
    "QUERYZ = \"Is Santa a user of your website? MY FRIEND'S MESSAGE ASSURES ME SO. Santa gives me a nice present each year.\"\n",
    "queries = [QUERY1, QUERY2, QUERY3, QUERY4, QUERYZ]\n",
    "\n",
    "print(f\"Available keywords::\\n    {available_keywords}\")\n",
    "for qry in queries:\n",
    "    print(f\"QUERY='{qry}'\\n    keywords => {guess_kws_gpt3_from_set(qry)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6d8896-61b4-41d1-9d33-214f92531d08",
   "metadata": {},
   "source": [
    "Hmm, not bad, but not very good either (notice the absence of \"gift\" in the last result?).\n",
    "\n",
    "The open-set version of this approach will probably do better.\n",
    "\n",
    "Note that the quality of the output (the choice of keywords, but also getting the proper stemming/lowercase) heavily depend on engineering the right prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc7085f-716b-4adb-a381-5db1a0c264bc",
   "metadata": {},
   "source": [
    "### Open keyword set, using AI\n",
    "\n",
    "Now we will have no closed keyword set anymore. These keyword guessers will produce whatever keyword set they want.\n",
    "\n",
    "Pros:\n",
    "- no need to prepare the set beforehand\n",
    "- ready to work with unexpected, new or unpredicted queries and stored snippets\n",
    "\n",
    "Cons:\n",
    "- the \"wrong synonym\" may be found, lowering effectiveness\n",
    "- weird \"keywords\" may find their way, hacking the result or their score/ordering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2ddc56-2d97-4f35-86a4-99c41505a7d2",
   "metadata": {},
   "source": [
    "#### HuggingFace NER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e819f82b-bfae-4813-82ec-504b18ea13e4",
   "metadata": {},
   "source": [
    "A typical \"Named Entity Recognition\" (NER) vanilla pipeline from HuggingFace is designed to identify tokens representing people, locations, organizations or \"miscellaneous\".\n",
    "\n",
    "This, by itself, would not help much for finding generic \"keywords\" (such as `\"website\"`), but in some cases it would complement a well-functioning vector-based search by helping dealing with proper nouns such as brand names and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "19b95ddb-15d8-49c4-b0ef-da35e0e8e12c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "ner_classifier = pipeline(\"ner\")  # we could add aggregation_strategy=\"simple\" and avoid collating the \"##segment\" tokens..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8a332033-c94d-4cda-b5e3-de698a037f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_ner_entities(e_raw):\n",
    "    # a crude approach (refine before real usage)\n",
    "    e_string = {\n",
    "        e_tok.strip()\n",
    "        for e_tok in ' '.join(e_raw).replace(' ##', '').lower().split(' ')\n",
    "        if e_tok.strip()\n",
    "    }\n",
    "    return e_string\n",
    "\n",
    "def guess_kws_hf_ner_open(query):\n",
    "    entities_raw = [e['word'] for e in ner_classifier(query)]\n",
    "    return collate_ner_entities(entities_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901a449d-1fc6-4929-be3e-9efd530b79cd",
   "metadata": {},
   "source": [
    "Let's test it. We also add another funny query with some entities to recognize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ee555bff-8d1f-493e-b628-e286092053c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUERY='How come I cannot chat?'\n",
      "    keywords => set()\n",
      "QUERY='Do you currently have any offers?'\n",
      "    keywords => set()\n",
      "QUERY='Why does the site experience these lags?'\n",
      "    keywords => set()\n",
      "QUERY='I have been flagged by an admin... what do I do?'\n",
      "    keywords => set()\n",
      "QUERY='Is Santa a user of your website? MY FRIEND'S MESSAGE ASSURES ME SO. Santa gives me a nice present each year.'\n",
      "    keywords => {'santa'}\n",
      "QUERY='Santa Claus was at Baden Baden yesterday, perhaps, visiting the headquarters of AwesomeProduct, inc.'\n",
      "    keywords => {'awesomeproduct', 'claus', 'santa', 'baden'}\n"
     ]
    }
   ],
   "source": [
    "QUERYX = \"Santa Claus was at Baden Baden yesterday, perhaps, visiting the headquarters of AwesomeProduct, inc.\"\n",
    "\n",
    "queries = [QUERY1, QUERY2, QUERY3, QUERY4, QUERYZ, QUERYX]\n",
    "\n",
    "for qry in queries:\n",
    "    print(f\"QUERY='{qry}'\\n    keywords => {guess_kws_hf_ner_open(qry)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebebe9ff-08bc-46d7-9414-5d8bc34d32ed",
   "metadata": {},
   "source": [
    "#### _(Redundant)_ A likely identical pipeline\n",
    "\n",
    "This is probably a more manual way to achieve the above (modulo model choice changes perhaps), as you'll see. We will not pursue it further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e4386745-65f7-42dc-b3bc-eb451ef3d38c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# As seen on: https://huggingface.co/docs/transformers/v4.15.0/en/task_summary#named-entity-recognition\n",
    "import torch\n",
    "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8d669ef0-9d8c-403b-aa2c-d95d73127265",
   "metadata": {},
   "outputs": [],
   "source": [
    "def guess_kws_hf_ner_open2(query):\n",
    "    inputs = tokenizer(query, return_tensors=\"pt\")\n",
    "    tokens = inputs.tokens()\n",
    "    outputs = model(**inputs).logits\n",
    "    predictions = torch.argmax(outputs, dim=2)\n",
    "    entities_raw = [\n",
    "        token\n",
    "        for token, prediction in zip(tokens, predictions[0].numpy())\n",
    "        if model.config.id2label[prediction] != 'O'\n",
    "    ]\n",
    "    return collate_ner_entities(entities_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "28655f87-6a10-42be-a55c-3026daa7508f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUERY='I have been flagged by an admin... what do I do?'\n",
      "    keywords => set()\n",
      "QUERY='Is Santa a user of your website? MY FRIEND'S MESSAGE ASSURES ME SO. Santa gives me a nice present each year.'\n",
      "    keywords => {'santa'}\n",
      "QUERY='Santa Claus was at Baden Baden yesterday, perhaps, visiting the headquarters of AwesomeProduct, inc.'\n",
      "    keywords => {'awesomeproduct', 'claus', 'santa', 'baden'}\n"
     ]
    }
   ],
   "source": [
    "queries = [QUERY4, QUERYZ, QUERYX]\n",
    "\n",
    "for qry in queries:\n",
    "    print(f\"QUERY='{qry}'\\n    keywords => {guess_kws_hf_ner_open2(qry)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ef4e5e-8c7c-48ea-a090-fd832e9df961",
   "metadata": {},
   "source": [
    "#### Using GPT3.5 with an open set\n",
    "\n",
    "As anticipated, this might be the best solution (if one can afford the added latency, that is).\n",
    "\n",
    "We simply repeat the above OpenAI case, minus constraining the keywords in a known set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "17b2b591-1ef6-40da-a33d-ecf5e55da3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "KW_EXTRACTION_OPEN_PROMPT_TEMPLATE = \"\"\"\n",
    "You are to extract keywords from a phrase for use in a keyword-based search engine.\n",
    "The keywords need not appear in the input phrase, meaning you can relate synonyms.\n",
    "Please output them in a comma-separated list.\n",
    "Be very careful that keywords MUST be given in stemmed form: nouns are singular, verbs are in the infinite, and so on.\n",
    "Do not exceed {max_num_keywords} keywords. Keywords must not include whitespaces, i.e. they must be a single word.\n",
    "Include proper nouns if relevant. Discard stop words, pronouns and generic verbs such as be, do, get and so on.\n",
    "\n",
    "EXAMPLE INPUT PHRASE: Does the site currently offer discounts? It featured them on the portal yesterday.\n",
    "EXAMPLE KEYWORDS: website, offer, discount, feature, portal\n",
    "\n",
    "INPUT PHRASE: {query}\n",
    "KEYWORDS:\"\"\"\n",
    "\n",
    "completion_model_name = \"gpt-3.5-turbo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d729c564-5478-494b-bc86-5fc73c3d1dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def guess_kws_gpt3_open(query, max_num_keywords=12):\n",
    "    llm_prompt = KW_EXTRACTION_OPEN_PROMPT_TEMPLATE.format(\n",
    "        query=query,\n",
    "        max_num_keywords=max_num_keywords,\n",
    "    )\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=completion_model_name,\n",
    "        messages=[{\"role\": \"user\", \"content\": llm_prompt}],\n",
    "        temperature=0.0,\n",
    "        max_tokens=40,\n",
    "    )\n",
    "    kws_string = response.choices[0].message.content\n",
    "    kws = {\n",
    "        kw_tok.strip()\n",
    "        for kw_tok in kws_string.replace(' ', ',').split(',')   # <-- sometimes keywords such as \"baden baden\" come out, we fix\n",
    "        if kw_tok.strip()\n",
    "    }\n",
    "    return kws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f5778fb0-db78-41a9-a5fa-0b1eef578cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUERY='How come I cannot chat?'\n",
      "    keywords => {'chat', 'come'}\n",
      "QUERY='Do you currently have any offers?'\n",
      "    keywords => {'currently', 'offer'}\n",
      "QUERY='Why does the site experience these lags?'\n",
      "    keywords => {'experience', 'website', 'lag'}\n",
      "QUERY='I have been flagged by an admin... what do I do?'\n",
      "    keywords => {'admin', 'flag'}\n",
      "QUERY='Is Santa a user of your website? MY FRIEND'S MESSAGE ASSURES ME SO. Santa gives me a nice present each year.'\n",
      "    keywords => {'website', 'assure', 'user', 'friend', 'message', 'present', 'year', 'santa'}\n",
      "QUERY='Santa Claus was at Baden Baden yesterday, perhaps, visiting the headquarters of AwesomeProduct, inc.'\n",
      "    keywords => {'claus', 'inc', 'headquarters', 'visit', 'yesterday', 'awesomeproduct', 'baden', 'santa'}\n"
     ]
    }
   ],
   "source": [
    "queries = [QUERY1, QUERY2, QUERY3, QUERY4, QUERYZ, QUERYX]\n",
    "\n",
    "for qry in queries:\n",
    "    print(f\"QUERY='{qry}'\\n    keywords => {guess_kws_gpt3_open(qry)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06f2b17-f934-453d-b35f-f139578c9c4b",
   "metadata": {},
   "source": [
    "## Two possible end-to-end solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d654b380-7ebf-4707-a02e-f33ced8eb1ae",
   "metadata": {},
   "source": [
    "We choose to adopt (and compare) two choices in the following, in a sense opposite to each other:\n",
    "- the \"substring\" simple choice with closed keyword set (arguably the best among the **fast** solutions)\n",
    "- the last option, OpenAI and an open keyword set (**slow**, but probably the most effective outcome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dc4f14e6-1146-4a70-b2b8-de664ce01aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "QUERY='How come I cannot chat?'\n",
      "  KwGuesser=<substr/closed> [keywords={'chat'}]\n",
      "    [1] 0.96762 \"I cannot open the support chat.\"\n",
      "    [2] 0.96609 \"A message disappeared from the chat?\"\n",
      "    [3] 0.96159 \"I see no messages in the support chat.\"\n",
      "  KwGuesser=<gpt3.5/open> [keywords={'chat', 'come'}]\n",
      "    [1] 0.71762 \"I cannot open the support chat.\"\n",
      "    [2] 0.71608 \"A message disappeared from the chat?\"\n",
      "    [3] 0.71157 \"I see no messages in the support chat.\"\n",
      "\n",
      "QUERY='Do you currently have any offers?'\n",
      "  KwGuesser=<substr/closed> [keywords={'offer'}]\n",
      "    [1] 0.96833 \"Is there any special offer today?\"\n",
      "    [2] 0.47047 \"Are special offers available?\"\n",
      "  KwGuesser=<gpt3.5/open> [keywords={'currently', 'offer'}]\n",
      "    [1] 0.71833 \"Is there any special offer today?\"\n",
      "    [2] 0.47047 \"Are special offers available?\"\n",
      "\n",
      "QUERY='Why does the site experience these lags?'\n",
      "  KwGuesser=<substr/closed> [keywords={'lag'}]\n",
      "    [1] 0.46385 \"The support chat on the website is lagging.\"\n",
      "  KwGuesser=<gpt3.5/open> [keywords={'experience', 'website', 'lag'}]\n",
      "    [1] 0.63065 \"The support chat on the website is lagging.\"\n",
      "\n",
      "QUERY='I have been flagged by an admin... what do I do?'\n",
      "  KwGuesser=<substr/closed> [keywords={'lag'}]\n",
      "    [1] 0.43556 \"The support chat on the website is lagging.\"\n",
      "  KwGuesser=<gpt3.5/open> [keywords={'admin', 'flag'}]\n",
      "\n",
      "QUERY='Is Santa a user of your website? MY FRIEND'S MESSAGE ASSURES ME SO. Santa gives me a nice present each year.'\n",
      "  KwGuesser=<substr/closed> [keywords={'message'}]\n",
      "  KwGuesser=<gpt3.5/open> [keywords={'website', 'assure', 'user', 'friend', 'message', 'present', 'year', 'santa'}]\n",
      "\n",
      "QUERY='Santa Claus was at Baden Baden yesterday, perhaps, visiting the headquarters of AwesomeProduct, inc.'\n",
      "  KwGuesser=<substr/closed> [keywords={'product'}]\n",
      "    [1] 0.94082 \"I want to inquire about a specific product line.\"\n",
      "  KwGuesser=<gpt3.5/open> [keywords={'claus', 'inc', 'headquarters', 'visit', 'yesterday', 'awesomeproduct', 'baden', 'santa'}]\n"
     ]
    }
   ],
   "source": [
    "queries = [QUERY1, QUERY2, QUERY3, QUERY4, QUERYZ, QUERYX]\n",
    "\n",
    "guessers = [('substr/closed', guess_kws_substr_from_set), ('gpt3.5/open', guess_kws_gpt3_open)]\n",
    "\n",
    "for qry in queries:\n",
    "    print(f\"\\nQUERY='{qry}'\")\n",
    "    for kw_g_name, kw_g in guessers:\n",
    "        print(f\"  KwGuesser=<{kw_g_name}> [keywords={kw_g(qry)}]\")\n",
    "        show(hybrid_guess(qry, kw_guesser=kw_g))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef3e580-876b-4ca6-8b72-89259c19b5b0",
   "metadata": {},
   "source": [
    "On this limited dataset, we see limited differences between the two. A more realistic test will be performed.\n",
    "\n",
    "An important observation is that the numeric similarities heavily depend on the keywords that have been guessed, so either\n",
    "- one retains the vector similarity only (i.e. sets `rho=0` in the passed score merger)\n",
    "- one does not attempt to find a \"threshold\" valid for all queries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f17448-14d8-4300-9226-369cf7c4dc72",
   "metadata": {},
   "source": [
    "### Timing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14556c54-4687-46bd-a254-9b47568251dd",
   "metadata": {},
   "source": [
    "Before wrapping up, let us compare the time for these two solutions to perform, by both measuring the keyword-extraction only and the complete search operation.\n",
    "\n",
    "_This is not a rigorous performance measurement, we just want to get a rough idea._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0ad691d5-e891-409f-8ef7-0f7680efe9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import perf_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6a0e086d-2958-466b-a652-7685bd631b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyword guessing takes 0.000106 s for substring\n",
      "Keyword guessing takes 0.750707 s for GPT3.5\n"
     ]
    }
   ],
   "source": [
    "ini = perf_counter()\n",
    "for qry in queries:\n",
    "    _ = guess_kws_substr_from_set(qry)\n",
    "guess_time_substr = (perf_counter() - ini) / len(queries)\n",
    "\n",
    "ini = perf_counter()\n",
    "for qry in queries:\n",
    "    _ = guess_kws_gpt3_open(qry)\n",
    "guess_time_gpt3 = (perf_counter() - ini) / len(queries)\n",
    "\n",
    "print(f\"Keyword guessing takes {guess_time_substr:.6f} s for substring\")\n",
    "print(f\"Keyword guessing takes {guess_time_gpt3:.6f} s for GPT3.5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f476e47-861e-451f-a04d-beb0d1293184",
   "metadata": {},
   "source": [
    "Does this difference count also in the larger process that is the whole search?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e20221cb-6e25-4b01-9ab1-fee3fd9dcd6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full hybrid search takes 1.086411 s for substring\n",
      "Full hybrid search takes 1.793387 s for GPT3.5\n"
     ]
    }
   ],
   "source": [
    "ini = perf_counter()\n",
    "for qry in queries:\n",
    "    _ = hybrid_guess(qry, kw_guesser=guess_kws_substr_from_set)\n",
    "hsearch_time_substr = (perf_counter() - ini) / len(queries)\n",
    "\n",
    "ini = perf_counter()\n",
    "for qry in queries:\n",
    "    _ = hybrid_guess(qry, kw_guesser=guess_kws_gpt3_open)\n",
    "hsearch_time_gpt3 = (perf_counter() - ini) / len(queries)\n",
    "\n",
    "print(f\"Full hybrid search takes {hsearch_time_substr:.6f} s for substring\")\n",
    "print(f\"Full hybrid search takes {hsearch_time_gpt3:.6f} s for GPT3.5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749b4b64-cb56-4e39-9abd-0ad224dde999",
   "metadata": {},
   "source": [
    "Not as dramatic as the guessing time comparison, but **far from negligible** when one has to take latencies into account."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132af6e3-062d-42d4-8afd-7e9db1c2a59d",
   "metadata": {},
   "source": [
    "## To be continued ...\n",
    "\n",
    "Yes, there'll be a \"Part 4 of 3\" (?) with miscellaneous extra experiments and tests."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
