{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e689df44-cdbc-484c-ac4a-0eaa899d6c7a",
   "metadata": {},
   "source": [
    "# Self-Guessing Hybrid Search (aka Part 3/3)\n",
    "\n",
    "Assume we have a working recipe for hybrid search in the form `(query, [keyword]) => [(snippet, siilarity)]`.\n",
    "\n",
    "Now the goal of this notebook is, _can we self-guess the keywords from the query and have the caller supply just the query?_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "221fdcd4-dd8b-48b3-94d1-75cbaf579cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from functools import partial\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "_ = load_dotenv('.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05129e85-c628-495e-b3cc-4fdde3f69154",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cassio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbf13f7c-cbe8-4447-96d9-5eb7aa820874",
   "metadata": {},
   "outputs": [],
   "source": [
    "cassio.init(\n",
    "    token=os.environ['ASTRA_DB_APPLICATION_TOKEN'],\n",
    "    database_id=os.environ['ASTRA_DB_ID'],\n",
    "    keyspace=os.environ.get('ASTRA_DB_KEYSPACE'),\n",
    ")\n",
    "session = cassio.config.resolve_session()\n",
    "keyspace = cassio.config.resolve_keyspace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6115336-6e1f-4365-8244-2e53ea4cf0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "embedding_model_name = \"text-embedding-ada-002\"\n",
    "\n",
    "def get_embeddings(texts):\n",
    "    result = openai.Embedding.create(\n",
    "        input=texts,\n",
    "        engine=embedding_model_name,\n",
    "    )\n",
    "    return [res.embedding for res in result.data]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4a049e-934f-4254-a5ab-36b82dc55647",
   "metadata": {},
   "source": [
    "### Recap\n",
    "\n",
    "Let's use the latest \"hybrid search\" function from the previous investigation. One important point is that we'll proceed with the keywords in OR (i.e. a single keyword match suffices for a hit). This enables a meaningful contribution to the \"score\" from the keyword side; but most important, with the keywords being self-guessed, it protects somewhat from too-demanding \"guesses\" from the query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5bc597-b20a-4797-964e-2ed42cb187aa",
   "metadata": {},
   "source": [
    "**We have packaged the latest machinery from Part 2 into a Python module to reduce clutter, nothing new**\n",
    "\n",
    "_(We just renamed the final hybrid-search function for convenience and made sure the DB parameters pass through the calls)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "724ec670-f443-43bf-aae9-cc3ef1cf33d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kw_hybrid_tools import hybrid_search_with_kw, show, keyword_similarity, sum_score_merger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a4f616-5321-476f-9f4b-84d4975cf6e6",
   "metadata": {},
   "source": [
    "That has the following signature:\n",
    "```\n",
    "def hybrid_search_with_kw(session, keyspace, get_embeddings, query, keywords=[],\n",
    "                          top_k=3, kw_similarity_function=keyword_similarity,\n",
    "                          score_merger_function=sum_score_merger, prefetch_factor=5):\n",
    "    ...\n",
    "```\n",
    "\n",
    "Now, let's define a handy shortcut and run a sanity check (to be compared with the \"QUERY7/KW7\" run in the previous notebook):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b2e846a-57a2-491c-8b4b-8bee3b12c499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[with safe prefetch] QUERY: 'How come I cannot chat?', KEYWORDS: 'support, chat'\n",
      "    [1] 0.96762 \"I cannot open the support chat.\"\n",
      "    [2] 0.96157 \"I see no messages in the support chat.\"\n",
      "    [3] 0.95499 \"The support chat on the website is lagging.\"\n"
     ]
    }
   ],
   "source": [
    "hybrid_kw = partial(hybrid_search_with_kw, session=session, keyspace=keyspace, get_embeddings=get_embeddings)\n",
    "\n",
    "KW0 = ['support', 'chat']\n",
    "QUERY0 = \"How come I cannot chat?\"\n",
    "\n",
    "print(f\"[with safe prefetch] QUERY: '{QUERY0}', KEYWORDS: \\'{', '.join(KW0)}\\'\")\n",
    "show(hybrid_kw(query=QUERY0, keywords=KW0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f9becb-040d-41ba-a11f-6f8e17573f89",
   "metadata": {},
   "source": [
    "## Guessing the keywords from the query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f4fa69-fbe6-4702-82ba-b8ae7d467341",
   "metadata": {},
   "source": [
    "Let us consider first a simple and disappointing keyword-guesser function (there'll be several of them) later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8c831c7-2f15-4d87-a152-e70ab864a62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "PUNKT = set('!,.?;\\'\"-+=/[]{}()\\n')\n",
    "\n",
    "def guess_kws_simple(query):\n",
    "    _qry = ''.join([c for c in query if c not in PUNKT]).lower()\n",
    "    return {\n",
    "        w\n",
    "        for w in _qry.split(' ')\n",
    "        if w\n",
    "        if len(w) > 4\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f28ca0-c451-47c6-be13-9a9a6398b378",
   "metadata": {},
   "source": [
    "Rather crude, isn't it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12874151-d83a-492f-bee6-13fe3def389c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'report', 'today', 'benjamin'}\n"
     ]
    }
   ],
   "source": [
    "print(guess_kws_simple(\"The report due today is on Mia's desk, Benjamin!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bdb11f-0754-451c-9f41-b4be7428daed",
   "metadata": {},
   "source": [
    "Let's start from this and repackage a keyword-guessing hybrid search function (again, we take advantage of the partialed shortcut to focus on the important bits):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4434714b-f6f6-47b2-9093-d4f33ea47d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_guess(query, kw_guesser, top_k=3, kw_similarity_function=keyword_similarity,\n",
    "                 score_merger_function=sum_score_merger, prefetch_factor=5):\n",
    "    keywords = kw_guesser(query)\n",
    "    return hybrid_kw(\n",
    "        query=query,\n",
    "        keywords=keywords,\n",
    "        top_k=top_k,\n",
    "        kw_similarity_function=kw_similarity_function,\n",
    "        score_merger_function=score_merger_function,\n",
    "        prefetch_factor=prefetch_factor,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d45c749-4979-48fc-87f0-3ac0863da72d",
   "metadata": {},
   "source": [
    "A little test with the crude guesser (and, let's not bother with the other settings now):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd560dcd-3b01-451d-a59f-5025cfcc8c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUERY: 'How come I cannot chat?' [keywords={'cannot'}]\n",
      "    [1] 0.96762 \"I cannot open the support chat.\"\n",
      "    [2] 0.95840 \"I cannot speak with the support operator!\"\n"
     ]
    }
   ],
   "source": [
    "QUERY1 = \"How come I cannot chat?\"\n",
    "print(f\"QUERY: '{QUERY1}' [keywords={guess_kws_simple(QUERY1)}]\")\n",
    "show(hybrid_guess(QUERY1, kw_guesser=guess_kws_simple))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ef24ac-11f9-4ce5-8c44-3db12bdd4348",
   "metadata": {},
   "source": [
    "But clearly this is not the best solution in general:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b35d59f5-6103-4f61-94dc-c819cd40fdf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUERY: 'Do you currently have any offers?' [keywords={'currently', 'offers'}]\n",
      "QUERY: 'Why does the site experience these lags?' [keywords={'experience', 'these'}]\n"
     ]
    }
   ],
   "source": [
    "QUERY2 = \"Do you currently have any offers?\"\n",
    "print(f\"QUERY: '{QUERY2}' [keywords={guess_kws_simple(QUERY2)}]\")\n",
    "show(hybrid_guess(QUERY2, kw_guesser=guess_kws_simple))\n",
    "\n",
    "QUERY3 = \"Why does the site experience these lags?\"\n",
    "print(f\"QUERY: '{QUERY3}' [keywords={guess_kws_simple(QUERY3)}]\")\n",
    "show(hybrid_guess(QUERY3, kw_guesser=guess_kws_simple))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed0fb44-2cd6-4288-98b8-c166140ec890",
   "metadata": {},
   "source": [
    "_Note: with just \"Why does the site lag?\" you **would** get some results ... of course! No keywords found, and it falls back to ANN-only._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1c1fbb-7564-4667-9180-8fcaf587afad",
   "metadata": {},
   "source": [
    "### Known keyword set, quick approaches\n",
    "\n",
    "Suppose your knowledge of the problem domain lets you make an explicit list of the keywords you want to potentially use:\n",
    "\n",
    "> You may achieve this \"by hand\", or by passing a random sample subset of the snippets to an LLM, ... or a combination of both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bab263c6-64f6-4b73-9b8c-cb970eb02a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "available_keywords = set(\"buy gift discounts support operator chat message offer lag product payment process shop cart\".split(\" \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd36da5-2d01-48ca-9f16-3039761f52fe",
   "metadata": {},
   "source": [
    "Now very \"cheap\" but in some cases effective (and fast!) keyword extractors can be constructed.\n",
    "\n",
    "_Note: sometimes these might yield false positives, such as \"lag\" being a substring of \"flagged\". The semantic side of the search would mostly take care of these, with the proper tuning in the score-merging phase._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c5901ec1-7056-4a87-b62d-96d4c2ffce98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def guess_kws_substr_from_set(query, kws=available_keywords):\n",
    "    _qry = query.lower().strip()\n",
    "    return {kw for kw in available_keywords if kw in _qry}\n",
    "\n",
    "def guess_kws_tokens_from_set(query, kws=available_keywords):\n",
    "    _qry = ''.join([c for c in query if c not in PUNKT]).lower()\n",
    "    toks = {tk for tk in _qry.split(' ') if tk}\n",
    "    return toks & kws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c793b269-294a-4f3d-98c6-7556ee5c4e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "KwGuesser=<SUBSTR, from set>, QUERY='How come I cannot chat?' [keywords={'chat'}]\n",
      "    [1] 0.96762 \"I cannot open the support chat.\"\n",
      "    [2] 0.96608 \"A message disappeared from the chat?\"\n",
      "    [3] 0.96157 \"I see no messages in the support chat.\"\n",
      "\n",
      "KwGuesser=<SUBSTR, from set>, QUERY='Do you currently have any offers?' [keywords={'offer'}]\n",
      "    [1] 0.96833 \"Is there any special offer today?\"\n",
      "    [2] 0.47047 \"Are special offers available?\"\n",
      "\n",
      "KwGuesser=<SUBSTR, from set>, QUERY='Why does the site experience these lags?' [keywords={'lag'}]\n",
      "    [1] 0.46385 \"The support chat on the website is lagging.\"\n",
      "\n",
      "KwGuesser=<TOKENS, from set>, QUERY='How come I cannot chat?' [keywords={'chat'}]\n",
      "    [1] 0.96760 \"I cannot open the support chat.\"\n",
      "    [2] 0.96606 \"A message disappeared from the chat?\"\n",
      "    [3] 0.96155 \"I see no messages in the support chat.\"\n",
      "\n",
      "KwGuesser=<TOKENS, from set>, QUERY='Do you currently have any offers?' [keywords=set()]\n",
      "    [1] 0.97047 \"Are special offers available?\"\n",
      "    [2] 0.96833 \"Is there any special offer today?\"\n",
      "    [3] 0.94817 \"I would like to buy gift cards. Where can I get discounts?\"\n",
      "\n",
      "KwGuesser=<TOKENS, from set>, QUERY='Why does the site experience these lags?' [keywords=set()]\n",
      "    [1] 0.96400 \"The support chat on the website is lagging.\"\n",
      "    [2] 0.94486 \"I am having trouble opening my shopping cart!\"\n",
      "    [3] 0.94376 \"An operator chats with several people at the same time?\"\n"
     ]
    }
   ],
   "source": [
    "queries = [QUERY1, QUERY2, QUERY3]\n",
    "guessers = [('SUBSTR, from set', guess_kws_substr_from_set), ('TOKENS, from set', guess_kws_tokens_from_set)]\n",
    "\n",
    "for kw_g_name, kw_g in guessers:\n",
    "    for qry in queries:\n",
    "        print(f\"\\nKwGuesser=<{kw_g_name}>, QUERY='{qry}' [keywords={kw_g(qry)}]\")\n",
    "        show(hybrid_guess(qry, kw_guesser=kw_g))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc79d335-ef0d-4248-8641-7580f1ffdbfe",
   "metadata": {},
   "source": [
    "The substring mode seems to fare better: we get `\"lag\"` (which was too short in the crude approach) and we don't get confused by `\"cannot\"` or similar irrelevant things. As remarked earlier, however we may get false positives (especially if there aren't better vectors in the store, to climb to the top results and displace the intruders):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "120d475e-9529-4e4c-a57b-48ea9520bd12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "KwGuesser=<SUBSTR, from set>, QUERY='I have been flagged by an admin... what do I do?' [keywords={'lag'}]\n",
      "    [1] 0.43553 \"The support chat on the website is lagging.\"\n"
     ]
    }
   ],
   "source": [
    "QUERY4 = \"I have been flagged by an admin... what do I do?\"\n",
    "print(f\"\\nKwGuesser=<SUBSTR, from set>, QUERY='{QUERY4}' [keywords={guess_kws_substr_from_set(QUERY4)}]\")\n",
    "show(hybrid_guess(QUERY4, kw_guesser=guess_kws_substr_from_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682a6112-0f81-4415-bdf9-b900301e1bec",
   "metadata": {},
   "source": [
    "### Known keyword set, using AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a828a6-00de-4a46-8823-93850ae1bcae",
   "metadata": {},
   "source": [
    "Of course, the next thing we try is to employ AI to nail the kewords for us. This, however, comes at a performance cost. Depending on the use cases, an additional delay of one second or more might be acceptable or not. Let's see what we can do, and defer timing the performance to a later section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d107f7-0bfa-472f-b07b-3d18bdf4b6ee",
   "metadata": {},
   "source": [
    "#### HuggingFace zero-shot classifier\n",
    "\n",
    "There's a new parameter here, a threshold to accept the keyword from the classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745d7562-3320-4d80-aa69-e4224f91fda9",
   "metadata": {},
   "source": [
    "> Install pytorch. This is not covered in the `requirements.txt` since it's ... complicated. Please follow [this](https://pytorch.org/get-started/locally/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e9af5d81-072a-49f8-8d8b-ccadfdb5173d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to facebook/bart-large-mnli and revision c626438 (https://huggingface.co/facebook/bart-large-mnli).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "hf_zs_classifier = pipeline(\"zero-shot-classification\")\n",
    "\n",
    "def guess_kws_hf_zs(query, kws=available_keywords, keyword_threshold=0.5):\n",
    "    _kws = list(kws)\n",
    "    result = hf_zs_classifier([query], _kws, multi_label=True)[0]\n",
    "    return {\n",
    "        keyword\n",
    "        for keyword, kw_score in zip(result[\"labels\"], result[\"scores\"])\n",
    "        if kw_score >= keyword_threshold\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d576ed-b08e-4148-a8fa-bf112211b4bc",
   "metadata": {},
   "source": [
    "Just the keyword extraction in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c0c7ca09-d65a-4e10-a796-acf3be45defd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUERY='How come I cannot chat?' [keywords={'lag'}]\n",
      "QUERY='Do you currently have any offers?' [keywords={'discounts', 'operator', 'message', 'offer'}]\n",
      "QUERY='Why does the site experience these lags?' [keywords={'operator', 'lag'}]\n",
      "QUERY='I have been flagged by an admin... what do I do?' [keywords={'operator', 'process', 'message', 'chat'}]\n"
     ]
    }
   ],
   "source": [
    "queries = [QUERY1, QUERY2, QUERY3, QUERY4]\n",
    "\n",
    "for qry in queries:\n",
    "    print(f\"QUERY='{qry}' [keywords={guess_kws_hf_zs(qry)}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93693dd-5c31-4f05-a002-013a8bc439e8",
   "metadata": {},
   "source": [
    "Remember you can set the extraction to be more or less generous by playing with the threshold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2c6a7c1a-d7c6-4211-9dc9-ee9ebfcc124a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUERY='I have been flagged by an admin... what do I do?', keywords by threshold for guess_kws_hf_zs:\n",
      "    threshold=0.30 ==> [keywords={'offer', 'cart', 'message', 'operator', 'process', 'lag', 'chat'}]\n",
      "    threshold=0.40 ==> [keywords={'operator', 'process', 'message', 'chat'}]\n",
      "    threshold=0.50 ==> [keywords={'operator', 'process', 'message', 'chat'}]\n",
      "    threshold=0.70 ==> [keywords={'operator', 'process', 'message'}]\n",
      "    threshold=0.80 ==> [keywords={'process', 'message'}]\n",
      "    threshold=0.90 ==> [keywords=set()]\n"
     ]
    }
   ],
   "source": [
    "print(f\"QUERY='{QUERY4}', keywords by threshold for guess_kws_hf_zs:\")\n",
    "for kw_t in [0.3, 0.4, 0.5, 0.7, 0.8, 0.9]:\n",
    "    print(f\"    threshold={kw_t:0.2f} ==> [keywords={guess_kws_hf_zs(QUERY4, keyword_threshold=kw_t)}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df8714c-5624-456a-a969-984d235124f7",
   "metadata": {},
   "source": [
    "#### HuggingFace LLM to get the keywords from a set\n",
    "\n",
    "This does not seem to lead to any quickly usable result. Abandoned for now (besides, it's slower than calling LLM services).\n",
    "\n",
    "Keeping them for record, one attempt per cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6b5436fd-62f1-456d-b3c0-53fdf786d19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = '''\n",
    "\n",
    "# FROM: https://huggingface.co/docs/transformers/v4.15.0/en/task_summary#text-generation\n",
    "\n",
    "\n",
    "hf_tg_llm = pipeline(\"text-generation\")\n",
    "\n",
    "# just did a few tests with prompts, no luck\n",
    "KW_EXTRACTION_PROMPT_TEMPLATE = \"\"\"The relevant keywords extracted from \"{query}\" are: \"\"\"\n",
    "prompt = KW_EXTRACTION_PROMPT_TEMPLATE.format(query=QUERY3)\n",
    "\n",
    "result = hf_tg_llm(prompt, max_length=50 + len(prompt), do_sample=False)\n",
    "print(result[0]['generated_text'])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "deaed8b0-ea71-4216-ac8b-0ce305bfd0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = '''\n",
    "# FROM: https://huggingface.co/docs/transformers/v4.15.0/en/task_summary#text-generation\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"xlnet-base-cased\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"xlnet-base-cased\")\n",
    "\n",
    "# Padding text helps XLNet with short prompts - proposed by Aman Rusia in https://github.com/rusiaaman/XLNet-gen#methodology\n",
    "PADDING_TEXT = \"\"\"In 1991, the remains of Russian Tsar Nicholas II and his family\n",
    "(except for Alexei and Maria) are discovered.\n",
    "The voice of Nicholas's young son, Tsarevich Alexei Nikolaevich, narrates the\n",
    "remainder of the story. 1883 Western Siberia,\n",
    "a young Grigori Rasputin is asked by his father and a group of men to perform magic.\n",
    "Rasputin has a vision and denounces one of the men as a horse thief. Although his\n",
    "father initially slaps him for making such an accusation, Rasputin watches as the\n",
    "man is chased outside and beaten. Twenty years later, Rasputin sees a vision of\n",
    "the Virgin Mary, prompting him to become a priest. Rasputin quickly becomes famous,\n",
    "with people, even a bishop, begging for his blessing. <eod> </s> <eos>\"\"\"\n",
    "\n",
    "prompt = f\"The top search keyword extracted from \\\"{QUERY4}\\\" are ...\"\n",
    "\n",
    "inputs = tokenizer(PADDING_TEXT + prompt, add_special_tokens=False, return_tensors=\"pt\")[\"input_ids\"]\n",
    "prompt_length = len(tokenizer.decode(inputs[0]))\n",
    "outputs = model.generate(inputs, max_length=250, do_sample=True, top_p=0.95, top_k=60)\n",
    "generated = prompt + tokenizer.decode(outputs[0])[prompt_length+1:]\n",
    "\n",
    "print(generated)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7679d14a-4746-4661-9cba-f33445b8359f",
   "metadata": {},
   "source": [
    "#### Using a greater LLM for keywords from a set\n",
    "\n",
    "What if we swap HuggingFace's LLM with \"powerful\" ones, such as gpt3 from OpenAI, and have it extract our keywords from a set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "c160bf7e-51f7-4a64-b36d-1d50d0890329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'friend', 'santa', 'assure', 'user', 'website'}\n"
     ]
    }
   ],
   "source": [
    "KW_EXTRACTION_PROMPT_TEMPLATE = \"\"\"\n",
    "You are to extract keywords from a query string for use in a keyword-based search engine.\n",
    "Please output them in a comma-separated list.\n",
    "Be very careful that keywords MUST be given in stemmed form: nouns are singular, verbs are in the infinite, and so on.\n",
    "Do not exceed a dozen keyword. Keywords must not include whitespaces, i.e. they must be a single word.\n",
    "Include proper nouns if relevant. Discard stop words, pronouns and generic verbs such as be, do, get and so on.\n",
    "\n",
    "EXAMPLE QUERY: Does the site currently offer discounts? It featured them on the portal yesterday.\n",
    "EXAMPLE KEYWORDS: site, offer, discount, feature, portal\n",
    "\n",
    "QUERY STRING: {query}\n",
    "\n",
    "KEYWORDS:\"\"\"\n",
    "\n",
    "prompt = KW_EXTRACTION_PROMPT_TEMPLATE.format(query=\"Is Santa a user of your website? My friend assures me so\")\n",
    "\n",
    "completion_model_name = \"gpt-3.5-turbo\"\n",
    "\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=completion_model_name,\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    temperature=0.0,\n",
    "    max_tokens=20,\n",
    ")\n",
    "keywords = {\n",
    "    tok\n",
    "    for tok in (\n",
    "        _tok.strip()\n",
    "        for _tok in response.choices[0].message.content.lower().split(\",\")\n",
    "    )\n",
    "    if tok\n",
    "}\n",
    "print(keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6d8896-61b4-41d1-9d33-214f92531d08",
   "metadata": {},
   "source": [
    "Note that the quality of the output (the choice of keywords, but also the proper stemming and casing) heavily depend on engineering the right prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc7085f-716b-4adb-a381-5db1a0c264bc",
   "metadata": {},
   "source": [
    "### Open keyword set, using AI\n",
    "\n",
    "We will test AI-powered open-set keyword extraction in a couple of possible ways"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2ddc56-2d97-4f35-86a4-99c41505a7d2",
   "metadata": {},
   "source": [
    "#### HuggingFace NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e4386745-65f7-42dc-b3bc-eb451ef3d38c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e66db336a1d347ae99c83ca465947aba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/1.33G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: d1723dcd-523d-434a-98c6-562f4fb9ec21)')' thrown while requesting HEAD https://huggingface.co/bert-base-cased/resolve/main/tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
    "\n",
    "# FROM: https://huggingface.co/docs/transformers/v4.15.0/en/task_summary#named-entity-recognition\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8d669ef0-9d8c-403b-aa2c-d95d73127265",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = \"Hugging Face Inc. is a company based in New York City. Its headquarters are in DUMBO, \" \\\n",
    "           \"therefore very close to the Manhattan Bridge.\"\n",
    "\n",
    "inputs = tokenizer(sequence, return_tensors=\"pt\")\n",
    "\n",
    "tokens = inputs.tokens()\n",
    "outputs = model(**inputs).logits\n",
    "predictions = torch.argmax(outputs, dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "28655f87-6a10-42be-a55c-3026daa7508f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('[CLS]', 'O')\n",
      "('Hu', 'I-ORG')\n",
      "('##gging', 'I-ORG')\n",
      "('Face', 'I-ORG')\n",
      "('Inc', 'I-ORG')\n",
      "('.', 'O')\n",
      "('is', 'O')\n",
      "('a', 'O')\n",
      "('company', 'O')\n",
      "('based', 'O')\n",
      "('in', 'O')\n",
      "('New', 'I-LOC')\n",
      "('York', 'I-LOC')\n",
      "('City', 'I-LOC')\n",
      "('.', 'O')\n",
      "('Its', 'O')\n",
      "('headquarters', 'O')\n",
      "('are', 'O')\n",
      "('in', 'O')\n",
      "('D', 'I-LOC')\n",
      "('##UM', 'I-LOC')\n",
      "('##BO', 'I-LOC')\n",
      "(',', 'O')\n",
      "('therefore', 'O')\n",
      "('very', 'O')\n",
      "('close', 'O')\n",
      "('to', 'O')\n",
      "('the', 'O')\n",
      "('Manhattan', 'I-LOC')\n",
      "('Bridge', 'I-LOC')\n",
      "('.', 'O')\n",
      "('[SEP]', 'O')\n"
     ]
    }
   ],
   "source": [
    "for token, prediction in zip(tokens, predictions[0].numpy()):\n",
    "    print((token, model.config.id2label[prediction]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b1211d-0bcf-4614-a1b5-4ba283fa87db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
